{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2> DS 3000 - Summer 2020</h2> </center>\n",
    "<center> <h3> DS Report </h3> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h3> Analysis and Prediction of Twitch Channel Growth</h3> </center>\n",
    "<center><h4>Carrine Shrestha, Yihang Li</h4></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executive Summary:\n",
    "\n",
    "Add your summary here (100-150 words)\n",
    "\n",
    "Provide a brief summary of your project. After reading this executive summary, your readers should have a rough understanding of what you did in this project. You can think of this summary in terms of the four sections of the report and write 1-2 sentences describing each section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "1. <a href='#1'>INTRODUCTION</a>\n",
    "2. <a href='#2'>METHOD</a>\n",
    "3. <a href='#3'>RESULTS</a>\n",
    "4. <a href='#4'>DISCUSSION</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement**\n",
    "\n",
    "With the growth of live streaming as an entertainment platform, coronavirus has presented a strong opportunity for online streaming platforms to expand and grow. We want to track this growth and how growth is displayed among the creators in Twitch. Understanding Twitch’s growth and live streaming as a career field is important to those looking into streaming as a viable at-home, virtual job, and predicting measures of growth and success. We will create an expected projection of Twitch channels over the next year on a monthly basis of viewership and other metrics, which will be used in parallel with subscriber data to project expected income. We want to gain insight into the jump in viewership due to coronavirus and better understand the rise of the streaming industry. \n",
    "\n",
    "**Significance of the Problem**\n",
    "\n",
    "Live streaming is an industry that is quickly growing in the technology space. Following Twitch’s initial success in the early 2010s, Youtube (via Youtube Live), Facebook (Facebook/Instagram Live) and other major platforms have followed up by creating their own streaming platforms for content creators. Insights into Twitch’s growth potential can quantify the growth of live streaming and their platforms as a brand new career field with opportunities for people to look into. Tracking the projected growth of a fast-moving industry in live streaming can be really useful for choosing a job as a software or data engineer, as joining one of these companies can be particularly lucrative. Alternatively, should investment opportunities present themselves in the future, having an understanding of YoY growth and metric-backed expectations can lead to potentially rewarding investments. Finally, if someone were to come across the expected growth and data and were interested in becoming a livestreamer themselves, they could use the projected data to measure their own metrics and success.\n",
    "\n",
    "**Previous Work on the Topic**\n",
    "\n",
    "Most of the analysis done around Twitch.tv’s data is in regards to the platform as a whole. Students from WPI found relationships between the demographics and the viewership behavior in relation to the types and formats of the content being delivered (Farrington and Muesch). Other breakdowns look at aggregate metrics for Twitch, such as the numbers of broadcasters and peak viewership in order to generate a tentative estimate of Twitch’s revenue (Iqbal). Even recommendations for marketing managers and potential influencers are presented with aggregate data metrics (Influencer Marketing Hub). Given the background of Twitch as a relatively new platform, we were unable to find any analysis on specific channels or the application of machine learning methods to predict singular channel growth. Identifying this gap and realizing its potential, we set out to create models that would allow us to explore Twitch through the lens of singular channels and learn from applying ML methods to real data.\n",
    "\n",
    "**Questions/Hypotheses**\n",
    "* **Research Questions**\n",
    "    * Which features are most important in predicting the expected future income of Twitch channels?\n",
    "    * What is the ideal number of features to include in analysis when predicting income of Twitch channels?\n",
    "    * Which supervised machine learning algorithm will produce the highest accuracy for our test dataset?\n",
    "\n",
    "* **Hypotheses 1**\n",
    "    * Hypotheses: The average viewer count is the most influential feature in predicting income. \n",
    "    * Null: The average viewer count has no effect on income. \n",
    "\n",
    "* **Hypotheses 2**\n",
    "    * Hypotheses: Stream time has a positive relationship with income. \n",
    "    * Null: Stream time has no relationship with income. \n",
    "\n",
    "* **Hypotheses 3**\n",
    "    * Hypotheses: A significant relationship exists between language and income. \n",
    "    * Null: No relationship exists between language and income. \n",
    "\n",
    "**References**\n",
    "\n",
    "Iqbal, Mansoor. “Twitch Revenue and Usage Statistics (2020).” Business of Apps, Sokomedia, 24 Apr. 2020, www.businessofapps.com/data/twitch-statistics/\n",
    "\n",
    "“25 Useful Twitch Stats for Influencer Marketing Managers [Infographic].” Influencer Marketing Hub, 10 June 2020, influencermarketinghub.com/twitch-stats/\n",
    "\n",
    "Bellanger, Caroline. “New Study from Upfluence Finds COVID-19 Lockdown Restrictions Resulted in a 24% Viewership Increase on Live-Streaming Platform Twitch.” Upfluence, Upfluence, 3 Apr. 2020, www.upfluence.com/press-release/new-study-from-upfluence-finds-covid-19-lockdown-restrictions-resulted-in-a-24-viewership-increase-on-live-streaming-platform-twitch\n",
    "\n",
    "Farrington, Daniel Jeffrey, and Muesch, Nicholas Matthew. “Analysis of the Characteristics and Content of Twitch Live-Streaming.” Digital WPI, Worcester Polytechnic Institute, Mar. 2015, digitalcommons.wpi.edu/iqp-all/3376/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Data Acquisition\n",
    "\n",
    "We sourced the data we are using ourselves through web-scraping two websites, TwitchTracker and SullyGnome, where both sites track and record channel metrics from Twitch.\n",
    "\n",
    "From **SullyGnome**, we were able to obtain data of the 100 most popular Twitch streamers per month. The data included information such as watch time, stream time, peak viewers, etc. and we sourced data for 13 months, ranging from April 2019 to April 2020. We obtained a total number of 11 variables from the dataset.\n",
    "\n",
    "We took subscriber data from **TwitchTracker** and used it to generate the channel’s income data, then appended the income data to a cleaned aggregate dataset from SullyGnome. This variable is the outcome variable. \n",
    "\n",
    "Twitch: https://twitchtracker.com/subscribers\n",
    "SullyGnome: https://sullygnome.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Variables\n",
    "\n",
    "**Feature Variables:**\n",
    "* Watch time\n",
    "* Stream Time\n",
    "* Peak Viewers\n",
    "* Average Viewers\n",
    "* Followers\n",
    "* Followers Gained\n",
    "* Views Gained\n",
    "* Partnered\n",
    "* Mature\n",
    "* Language\n",
    "* Income\n",
    "\n",
    "**Target Variable:**\n",
    "* Income\n",
    "\n",
    "For our analysis purposes, we used the feature variables as independent variables and the target variable as the dependent variable for all our hypotheses. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Data Analysis\n",
    "**Feature Extraction:** We will use Iterative Feature Selection to gain an understanding of which variables are important and the order of importance of the variables. This will allow us to gain more insight into our data. For analysis, we will use Model-Based Feature Selection to judge which features are most important for use in our model. \n",
    "\n",
    "**Machine Learning Techniques:** We will be applying supervised machine learning algorithms to analyze the data. As our target variable of future income is a continuous variable, we plan on using regression analysis on our feature variables. We will be applying Multiple Linear Regression, Lasso, Ridge, KNNRegressor and SVM machine learning algorithms to our data to analyze for best results as they are all used to tune regression models. We expect Lasso to perform the best as our data contains many features, but we expect only a few of them to be impactful to the target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Wrangling\n",
    "\n",
    "\n",
    "**Simple Data Cleaning:**\n",
    "\n",
    "For the data obtained from SullyGnome, we merged all of the files from each month together. In order to be able to sufficiently train our machine for each user, we decided that we need to obtain at least 10 data points per channel so we omitted all users who did not appear in the dataset at least 10 times. Next, we adjusted the channel names as, in some instances, the foreign name and the English name of the channel would appear in the data. We cleaned it so that only the English name appeared, which would then be used by the subscriber scraper. \n",
    "\n",
    "For the subscriber scraper, we scraped data from the Twitch site for each user. The scraping takes about 2-3 hours. From that information, we calculated the current subscriber based income of the users by performing basic multiplication based on the subscriber count per tier. \n",
    "\n",
    "The final dataset will merge the total income variable from the Twitch data and the data from SullyGnome. The variables will merge together on the accounts of channel name and data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "allchannels = pd.read_csv(\"generated_data/TotalChannelData.csv\")\n",
    "allchannels.set_index(\"Channel\", inplace=True)\n",
    "\n",
    "subdata = pd.read_csv(\"generated_data/SubData.csv\")\n",
    "subdata.set_index(\"Channel\", inplace=True)\n",
    "\n",
    "finaldata = pd.merge(allchannels, subdata, how='left', on=['Channel', 'Month'])\n",
    "finaldata.drop(['Unnamed: 0', \"Prime\", \"Tier 1\", \"Tier 2\", \"Tier 3\", \"Total\", \"Unshared\"], axis =1, inplace=True)\n",
    "\n",
    "#removing all rows where total income is not provided, meaning subscriber data was not scraped \n",
    "#this means the rows being removed are all channels that have less than 10 datapoints\n",
    "finaldata = finaldata[finaldata['Total Income'].notna()]\n",
    "\n",
    "#removing dollar formatting from total income \n",
    "finaldata['Total Income'] = finaldata['Total Income'].str.replace(',', '').str.replace('$', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any partnered channels that are \"False\"\n",
    "def check_continuous_variables(df, col):\n",
    "    false_count = 0\n",
    "    for i in range(len(finaldata.loc[:,col])):\n",
    "        if finaldata.loc[:,col][i] == False:\n",
    "            false_count +=1 \n",
    "        else:\n",
    "            continue\n",
    "    print(\"There are\", false_count, \"false values for\", col)\n",
    "    if false_count == 0:\n",
    "        finaldata.drop([col], axis = 1, inplace=True)\n",
    "    return finaldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldata = check_continuous_variables(finaldata, \"Partnered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldata = check_continuous_variables(finaldata, \"Mature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldata = finaldata.sort_values(by=[\"Channel\", \"Month\"], ascending=True)\n",
    "\n",
    "finaldata[\"Income Change\"] = 0\n",
    "\n",
    "channels = finaldata.index.unique()\n",
    "\n",
    "for channel in channels:\n",
    "    channeldata = finaldata[finaldata.index == channel]\n",
    "    print(channel)\n",
    "    prev = 0\n",
    "    for index, row in channeldata.iterrows():\n",
    "        if(prev != 0):\n",
    "            row[\"Income Change\"] = row[\"Total Income\"] - prev\n",
    "        prev = row[\"Total Income\"]\n",
    "        print(row[\"Month\"], row[\"Total Income\"], row[\"Income Change\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returns features and target variables \n",
    "def features_target(df, target_var):\n",
    "    features = df.drop(target_var, axis = 1)\n",
    "    features = features.reset_index() #index is reset to merge with encoded features \n",
    "    target = df[target_var]\n",
    "   \n",
    "    return features, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = features_target(finaldata, \"Total Income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#one-hot encoding for \"Mature\" and \"Language\" Variables \n",
    "encode_features = features.iloc[:, 8:10]\n",
    "\n",
    "encoder = OneHotEncoder(sparse = False)\n",
    "encoded_df = encoder.fit_transform(encode_features)\n",
    "\n",
    "features_df = pd.DataFrame(encoded_df, columns = encoder.get_feature_names())\n",
    "\n",
    "#merging both features datasets together \n",
    "features = features_df.merge(features, left_index = True, right_index = True)\n",
    "features.drop([\"Mature\", \"Language\"], axis = 1, inplace=True)\n",
    "\n",
    "#removing channel and month as a variables as they work as identifiers \n",
    "features.reset_index()\n",
    "features.drop([\"Channel\", \"Month\"], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Iterative Feature Selection to return a list of features in order of importance \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "def ife_features(X_train, y_train, X_test, y_test):\n",
    "    name_features = list(X_train.columns.values)\n",
    "    num_features = len(name_features)\n",
    "    features_names = []\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        i += 1\n",
    "        select = RFE(DecisionTreeRegressor(random_state = 3000), n_features_to_select = i)\n",
    "        \n",
    "        #fit the RFE selector to the training data\n",
    "        select.fit(X_train, y_train)\n",
    "        \n",
    "        #transform training and testing sets so only the selected features are retained\n",
    "        X_train_selected = select.transform(X_train)\n",
    "        X_test_selected = select.transform(X_test)\n",
    "        \n",
    "        selected_features = select.get_support()\n",
    "        \n",
    "        #adding to the list of features in order of appearance / importance \n",
    "        for x in range(num_features):\n",
    "            if selected_features[x] == True:\n",
    "                if name_features[x] in features_names:\n",
    "                    continue\n",
    "                else:\n",
    "                    features_names.append(name_features[x])\n",
    "    print(\"Order of Importance of Features:\")\n",
    "    order = 1\n",
    "    for value in features_names:\n",
    "        print(\"\\t\",order,value)\n",
    "        order+= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifefeatures = ife_features(X_train, y_train, X_test, y_test)\n",
    "ifefeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def modelbased_features(X_train, y_train, X_test, y_test, analysis_type):\n",
    "    \n",
    "    name_features = list(X_train.columns.values)\n",
    "    \n",
    "    select = SelectFromModel(DecisionTreeRegressor(random_state = 3000), threshold = analysis_type)\n",
    "\n",
    "    select.fit(X_train, y_train)\n",
    "    X_train_selected = select.transform(X_train)\n",
    "    X_test_selected = select.transform(X_test)\n",
    "\n",
    "    selected_features = select.get_support()\n",
    "    feature_names = []\n",
    "    for x in range(len(selected_features)):\n",
    "        if selected_features[x] == True:\n",
    "            feature_names.append(name_features[x])\n",
    "            \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelbasedfeatures = modelbased_features(X_train, y_train, X_test, y_test, 'median')\n",
    "modelbasedfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Data Exploration\n",
    "* Generate appropriate data visualizations for your key variables identified in the previous section\n",
    "* You should have at least three visualizations (and at least two different visualization types)\n",
    "* For each visualization provide an explanation regarding the variables involved and an interpretation of the graph.\n",
    "* If you are using Plotly, insert your visualizations as images as well (upload the graph images to an online source, e.g. github, and embed those into the cells in Jupyter Notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(finaldata, x=\"Followers\", y=\"Total Income\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(finaldata, x=\"Total Income\", nbins=50)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(finaldata, y=\"Total Income\", x=\"Month\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(finaldata, x=\"Watch time\", y=\"Total Income\", title=\"Watch Time vs Total Income\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(finaldata, x=\"Stream time\", y=\"Total Income\", title=\"Stream Time vs Total Income\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Model Construction\n",
    "* Conduct your hypothesis test(s) here.\n",
    "* For your machine learning question(s), use the Training, Validation, and Testing approach through GridSearch\n",
    "* Apply machine learning algorithms (apply at least three different algorithms)\n",
    "* Train your algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "def oneway_ANOVA(data, IV, DV):\n",
    "    oneway = stats.f_oneway(data[IV], data[DV])\n",
    "    fstat = oneway[0]\n",
    "    pval = oneway[1]\n",
    "    df1 = len(data.groupby(IV)) - 1\n",
    "    df2 = len(data.groupby(DV)) - 1\n",
    "    #levene_result = stats.levene(auto_sug, dynamic, explicit)\n",
    "    #Execute output statements\n",
    "    print(\"-----------------------\")\n",
    "    print(\"ONE-WAY ANOVA RESULTS\")\n",
    "    print(\"-----------------------\\n\\n\")\n",
    "    print(\"F-test\")\n",
    "    print(\"-------\\n\")\n",
    "    print(\"F(%d,%d) = %.2f, p = %s\" %(df1, df2, fstat, pval))\n",
    "#     print(\"\\n\\nAssumption Checks\")\n",
    "#     print(\"-------------------\\n\")\n",
    "#     print(\"Assumption of Equality of Variances:\")\n",
    "#     print(\"\\t\" + str(levene_result))\n",
    "#     print(\"\\t\", end=\"\")\n",
    "#     checkAssumption(levene_result[1])\n",
    "    print(\"\\nAssumption of Normality:\")\n",
    "    shapiro_test_IV = stats.shapiro(data[IV])\n",
    "    print(\"\\t\" + IV + \": \" + str(shapiro_test_IV))\n",
    "    print(\"\\t\\t\", end=\"\")\n",
    "    checkAssumption(shapiro_test_IV[1])\n",
    "    shapiro_test_DV = stats.shapiro(data[DV])\n",
    "    print(\"\\t\" + DV + \": \" + str(shapiro_test_DV))\n",
    "    print(\"\\t\\t\", end=\"\")\n",
    "    checkAssumption(shapiro_test_DV[1])\n",
    "#     checkAssumption(shapiro_dynamic[1])\n",
    "#     print(\"\\tExplicit : \" + str(shapiro_explicit))\n",
    "#     print(\"\\t\\t\", end=\"\")\n",
    "#     checkAssumption(shapiro_explicit[1])\n",
    "#     print(\"\\tAuto-suggestion : \" + str(shapiro_auto))\n",
    "#     print(\"\\t\\t\", end=\"\")\n",
    "#     checkAssumption(shapiro_auto[1])\n",
    "    \n",
    "#     mc = MultiComparison(data[DV], data[IV])\n",
    "#     tukey_result = mc.tukeyhsd()\n",
    "#     print(\"\\n\\nPost-hoc Tests\")\n",
    "#     print(\"----------------\\n\")\n",
    "#     print(tukey_result)\n",
    "    \n",
    "#Print the correct assumption statement\n",
    "def checkAssumption(pval):\n",
    "    if pval > 0.05:\n",
    "        print(\"Assumption is met. p > .05\")\n",
    "    else:\n",
    "        print(\"Assumption is violated. p < .05\")\n",
    "        \n",
    "oneway_ANOVA(finaldata, \"Peak viewers\", \"Total Income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(list(finaldata.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\"Linear Regression\": LinearRegression(), \"Ridge\": Ridge(), \"Lasso\":Lasso(), \"k-Nearest Neighbor\":\n",
    "             KNeighborsRegressor(), \"Support Vector Machine\": LinearSVR(max_iter=10000000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressors_percentage_split():\n",
    "    \n",
    "    for value in estimators:\n",
    "        model = estimators[value].fit(X=X_train, y=y_train)\n",
    "        r_train = r2_score(y_train, model.predict(X_train))\n",
    "        r_test = r2_score(y_test, model.predict(X_test))\n",
    "        print(value+\":\")\n",
    "        print(\"\\tR-squared value for training set: \",r_train)\n",
    "        print(\"\\tR-squared value for testing set: \",r_test,\"\\n\" )\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors_percentage_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Model Evaluation\n",
    "* Evaluate the performance of your algorithms on appropriate evaluation metrics, using your validation set\n",
    "    * Use at least two different metrics \n",
    "* Evaluate your results from multiple ML models and hypothesis tests\n",
    "    * What was the performance of each algorithm in plain English? Is there any indication of overfitting/underfitting?\n",
    "    * Was there a significant difference? Use the template from lecture slides when reporting the results of your hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Model Optimization\n",
    "* Tune your models using appropriate hyperparameters\n",
    "* Explain why you are doing this (e.g., to avoid overfitting, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Model Testing\n",
    "* Test your tuned algorithms using your testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DISCUSSION\n",
    "* Provide a summary of the steps you took to analyze your data and test your predictive model\n",
    "* Interpret your findings from 3.4., 3.5, and 3.6\n",
    "    * Which algorithms did you compare?\n",
    "    * Which algorithm(s) revealed best performance?\n",
    "    * Which algorithm(s) should be used for your predictive model?\n",
    "    * Based on your findings, can we you the features in your dataset to predict the outcome variable you identified using the algorithms you've applied? (It is okay if the answer is no. We're interested in the process, not the performance of the model.)\n",
    "* For your hypotheses, interpret the results. What does it mean to have significant/non-significant differences with regards to your data?\n",
    "\n",
    "\n",
    "* End this section with a conclusion paragraph containing some pointers for future work\n",
    "    * (e.g., get more data/features, perform another analysis, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTRIBUTIONS\n",
    "* Describe each team member's contributions to the report (who did what in each section)\n",
    "* Remember this is a team effort!\n",
    "* Each member of your team will provide peer evaluation of other team members. Your final grade on the project will be based on those peer evaluations. An survey will be shared after the deadline for this deliverable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
